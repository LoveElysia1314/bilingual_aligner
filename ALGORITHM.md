[中文](ALGORITHM_CN.md) | English

# Bilingual Alignment Algorithm: Single-Stage Dynamic Programming with Structural Validation

## Table of Contents

1. Problem Formalization
2. Mathematical Modeling
3. Algorithm Architecture
4. Constraint Node Computation
5. Single-Stage Dynamic Programming
6. Post-Processing Repairs
7. Complexity Analysis
8. Parameter Configuration

---

## Problem Formalization

### Input Data Structure

Given two parallel text sequences:
- Source text: $\text{src}[1..n]$, where each element $\text{src}[i]$ is a `LineObject` instance
- Target text: $\text{tgt}[1..m]$, where each element $\text{tgt}[j]$ is a `LineObject` instance

Each `LineObject` contains:
- Text content
- Word vector embedding $\mathbf{v} \in \mathbb{R}^d$
- Punctuation features
- Original line number information

### Alignment Graph Model

Define the alignment graph $G = (V, E)$, where:
- Vertex set $V = \{(i,j) : 0 \le i \le n, 0 \le j \le m\}$
- Edge set $E$ contains three types of operation edges:
  1. **1:1 operation** (`NO_SHIFT`): $(i,j) \to (i+1,j+1)$
  2. **2:1 operation** (`SOURCE_AHEAD`): $(i,j) \to (i+2,j+1)$
  3. **1:2 operation** (`TARGET_SPLIT`): $(i,j) \to (i+1,j+2)$

### Optimization Objective

Find the path $\pi^*$ from $(0,0)$ to $(n,m)$ that maximizes the total similarity (sum of edge weights):

$$\pi^* = \arg\max_{\pi \in \Pi} \sum_{e \in \pi} w(e)$$

where $w(e)$ is the edge similarity weight, and $|\pi|$ is the number of operations in the path. The average similarity $\text{avg\_sim}(\pi)=\text{score}(\pi)/|\pi|$ is computed and reported after path extraction, but the implemented DP optimizes the total sum.

---

## Mathematical Modeling

### Similarity Function

For any source line set $I \subseteq \{1,\ldots,n\}$ and target line set $J \subseteq \{1,\ldots,m\}$, define the similarity function:

$$\text{sim}(I,J) = \frac{1}{|I| \times |J|} \sum_{i \in I} \sum_{j \in J} \text{cosine}(\mathbf{v}_i, \mathbf{v}_j)$$

Similarity is calculated using cosine similarity, based on sentence embeddings generated by sentence-transformers.

### Edge Weight Computation

For edge $e = ((i,j), (i+\Delta i, j+\Delta j))$, its weight is the average similarity of all line pairs covered by the corresponding operation:

$$w(e) = \text{sim}\left(\{i,\ldots,i+\Delta i-1\}, \{j,\ldots,j+\Delta j-1\}\right)$$

Regardless of operation type (1:1, 2:1, or 1:2), all edge weights use uniform computation.

### Path Score Computation

For path $\pi = [e_1, e_2, \ldots, e_k]$, the total score is:

$$\text{score}(\pi) = \sum_{t=1}^k w(e_t)$$

The average similarity is reported as:

$$\text{avg\_sim}(\pi) = \frac{\text{score}(\pi)}{k}$$

Note: the implemented DP searches for the path that maximizes $\text{score}(\pi)$ (total sum); the average similarity is a post-hoc metric for reporting and quality assessment.

---

## Algorithm Architecture

### Overall Flow

```
Input: src_lines, tgt_lines (List[LineObject])
    ↓
[Stage 0: Constraint Node Computation]
├── Compute hard-constraint nodes S_hard
├── Apply node-level soft constraints S'
└── Compute stable reachable node set S*
    ↓
[Stage 1: Single-Stage Dynamic Programming]
├── DP computation of maximum cumulative scores
├── Backtrack optimal path from terminal
└── Extract alignment steps
    ↓
[Stage 2: Final Structural Validation]
└── Detect consecutive different non-1:1 operations in final path
    ↓
[Post-Processing: Repair Application]
├── Identify non-1:1 alignment positions
├── Generate repair candidates (split/merge/insert)
├── Sort and apply repairs
├── Detect alignment exceptions
└── Validate repair results
    ↓
Output: Alignment path, repair logs, and statistics
```

### Core Components

1. **`EnumPruningAligner`** (enum_aligner.py): Core alignment algorithm, executes constraint node computation and single-stage DP
2. **`BilingualCorpus`** (`corpus.py`): Corpus management and similarity computation
3. **`RepairExecutor`** (executor.py): Post-processing repair execution
4. **`TextAligner`** (repairer.py): Main coordinator, integrates alignment and repair processes

---

## Constraint Node Computation

### Hard Constraint Definition

Hard constraints are based on text length proportions, defining the feasible search space:

$$S_{\text{hard}} = \left\{ (i,j) : 
\begin{cases}
\left\lceil \frac{i}{2} \right\rceil \le j \le 2i & \text{(first half constraint)} \\
\left\lceil \frac{n-i}{2} \right\rceil \le m-j \le 2(n-i) & \text{(second half constraint)}
\end{cases}
\right\}$$

This constraint assumes the length ratio between source and target texts varies approximately between 1:2 and 2:1, allowing up to ±50% deviation.

### Node-Level Soft Constraints

Further filter hard-constraint nodes using node-level filtering:

1. **Compute 1:1 reference similarities**: For all possible 1:1 operations $(i,i+1) \to (j,j+1)$, compute raw similarity $\text{raw}(i,j)$
   - Record $\text{src\_best}[i] = \max_j \text{raw}(i,j)$ (best 1:1 match score for source line $i$)
   - Record $\text{tgt\_best}[j] = \max_i \text{raw}(i,j)$ (best 1:1 match score for target line $j$)

2. **Compute node best output score**: For each node $(i,j)$, compute the maximum similarity across all possible outgoing edges (1:1, 2:1, 1:2):
   $$\text{node\_score}(i,j) = \max\{\text{raw}(i..i+\Delta i-1, j..j+\Delta j-1) : (i+\Delta i, j+\Delta j) \in S_{\text{hard}}\}$$

3. **Apply relative threshold**: Determine the best 1:1 reference score involved in this node.

   In the current implementation we use a conservative combination rule: when both
   source- and target-side per-line bests are available, we take the smaller of the
   two (i.e. the weaker side) as the node reference; if only one side is available,
   that side's best is used. Formally:

   $$\text{element\_best}(i,j) = \begin{cases}
   \min(\text{src\_best}[i], \text{tgt\_best}[j]) & \text{if both available} \\
   	ext{src\_best}[i] & \text{if only source available} \\
   	ext{tgt\_best}[j] & \text{if only target available}
   \end{cases}$$

4. **Filter weak nodes**: If $\text{node\_score}(i,j) < \theta \cdot \text{element\_best}(i,j)$, remove this node. Here $\theta$ is the node-level relative threshold (default 0.8).

### Stable Reachable Node Set

Combine hard constraints, soft constraints, and reachability analysis:

1. **Forward reachability**: Set of nodes reachable from $(0,0)$ via allowed operations $R_{\text{forward}}$
2. **Backward reachability**: Set of nodes that can reach $(n,m)$ in reverse $R_{\text{backward}}$
3. **Stable node set**:
   $$S^* = S_{\text{hard}} \cap S_{\text{soft}} \cap R_{\text{forward}} \cap R_{\text{backward}}$$

Iteratively apply forward/backward filtering until convergence, ensuring all nodes are reachable from the start and can reach the end.

### Notes on node-level policy
- Using `min(src_best, tgt_best)` makes the node-level threshold conservative: the
   weaker side determines the standard, which tends to retain nodes in heterogeneous
   or asymmetric cases. If either side has no 1:1 reference (value 0), the node is
   conservatively kept (filtering skipped) to avoid accidental deletion of valid
   alignment anchors.

---

## Single-Stage Dynamic Programming

### DP Definition

For each stable node $(i,j) \in S^*$, define the maximum cumulative score:

$$\text{dp}[i][j] = \max \begin{cases}
\text{dp}[i-1][j-1] + w((i-1,j-1) \to (i,j)) & \text{if } (i-1,j-1) \in S^* \\
\text{dp}[i-2][j-1] + w((i-2,j-1) \to (i,j)) & \text{if } (i-2,j-1) \in S^* \\
\text{dp}[i-1][j-2] + w((i-1,j-2) \to (i,j)) & \text{if } (i-1,j-2) \in S^*
\end{cases}$$

Boundary condition: $\text{dp}[0][0] = 0$

### DP Computation Process

Compute using topological order: group nodes by $\text{level} = i + j$, processing from small to large. For each node, record its parent node and maximum score reaching it.

### Path Reconstruction

Start from terminal $(n,m)$, backtrack parent pointers to $(0,0)$, obtaining the node sequence. Convert adjacent node pairs to alignment operations.

### Average Similarity and Quality Metrics

The average similarity of the optimal path is:

$$\text{avg\_sim}^* = \frac{\text{dp}[n][m]}{|\pi^*|}$$

Here $\text{dp}[n][m]$ is the total similarity of the path found by DP; again DP optimizes total similarity, and $\text{avg\_sim}^*$ is derived afterwards for reporting.

### Optimization Guarantee

Since DP computation is consistent for all stable nodes, and forward/backward reachability filtering ensures graph connectivity, this DP finds the optimal path satisfying all constraints.

---

## Final Structural Validation

Perform structural checks on the final alignment path; **dynamic penalties are not applied during the search stage**.

### Exception Detection Rules

Scan all operations in the final path to detect consecutive different non-1:1 operations:

For two adjacent non-1:1 operations $op_a$ and $op_b$ in the operation sequence (neither is NO_SHIFT):
- If operation types differ (one is 2:1, the other is 1:2)
- And the distance between them $d \le L$ (lookahead window, default $L=5$)

Mark as **alignment exception** and report in repair logs.

This design ensures fairness during search (all operation types treated equally) while identifying potential structural issues for subsequent handling.

---

## Post-Processing Repairs

### Repair Objectives

Convert DP-identified non-1:1 alignment positions to 1:1 alignments (or mark as exceptions) through local text operations:
- **2:1 operations** → Source line splitting or target line insertion
- **1:2 operations** → Target line merging

### Repair Candidate Generation

#### 2:1 Operations (SOURCE_AHEAD)

For the case where source lines $\{i, i+1\}$ correspond to target line $j$, generate the following repair candidates:

1. **Hard split candidates**: Split source lines at punctuation boundaries, no semantic similarity adjustment required
   
2. **Soft split candidates**: Split at non-punctuation positions based on semantic similarity
   - Scoring method: $\text{split\_score} = \frac{\text{sim}(\text{part}_1, j) + \text{sim}(\text{part}_2, j)}{2}$
   - Choose the split position that maximizes total similarity

3. **Insert placeholder line**: As a fallback, insert a placeholder line (marked as synthetic) on the target side
   - Placeholder line's `original_line_number = -1`
   - Placeholder lines do not participate in subsequent global similarity calculations

#### 1:2 Operations (TARGET_SPLIT)

For the case where source line $i$ corresponds to target lines $\{j, j+1\}$:
- Generate **merge candidates**: Merge two target lines into one
- Merging method: Simple string concatenation or other predefined rules

### Repair Application Strategy

1. **Descending order sorting**: Process repair candidates by target line index from large to small to avoid index drift during repairs

2. **Incremental updates**: After each repair application, rebuild the corpus index mapping (`rebuild_index_mapping`)

3. **Validation checks**: Ensure source and target line counts are consistent in the repaired corpus, and index mappings are correct

### Quality Metrics and Statistics

During repairs, record:
- Similarity improvement before and after each repair
- Repair types (split/merge/insert)
- Source/target line ranges involved in repairs

Final repair statistics output:
- Total number of repairs
- Counts by repair type
- Overall similarity improvement
- 1:1 alignment ratio

---

## Complexity Analysis

### Time Complexity

#### Constraint Computation
- Hard constraint enumeration: $O(nm)$
- Node-level soft constraint filtering: $O(|S_{\text{hard}}| \times \text{avg\_neighbors})$, $O(nm)$ on sparse graphs
- Forward/backward BFS: $O(|S| + |E|) \le O(nm)$ each time, at most constant iterations
- **Total**: $O(nm)$

#### Single-Stage DP
- DP computation: $O(|S^*|) \le O(nm)$
- Path reconstruction: $O(\max(n,m))$
- **Total**: $O(nm)$

#### Post-Processing Repairs
- Repair candidate generation: $O(\text{\# non-1:1 operations} \times \text{\# candidates})$
- Repair sorting and application: $O(\text{\# repairs} \times \log \text{\# repairs})$
- **Total**: $O(\text{\# repairs} \cdot \log \text{\# repairs})$

#### Overall Complexity
$$T = O(nm + \text{\# repairs} \cdot \log \text{\# repairs})$$

In practice, the number of non-1:1 operations is usually much less than $nm$, so actual time complexity approaches $O(nm)$.

### Space Complexity

- DP table and parent pointers: $O(|S^*|) \le O(nm)$
- Edge weight cache: $O(|E|) \le O(3|S^*|)$
- Repair logs and corpus copies: $O(n + m)$
- **Total**: $O(nm)$

---

## Parameter Configuration

### Core Parameters

| Parameter | Default | Description | Applicable Stage |
|-----------|---------|-------------|------------------|
| `node_relative_threshold` | 0.8 | Relative threshold for node-level soft constraints | Stage 0 |
| `consecutive_non_1to1_lookahead` | 5 | Lookahead window size for structural validation | Stage 2 |
| `soft_split_penalty` | 0.05 | Penalty for soft splits between sentences (retained) | Repair |
| `insert_fallback_score` | 0.6 | Fixed similarity score for inserted placeholders | Repair |
| `delete_penalty` | 0.05 | Penalty for delete operations (reserved) | Repair |

### Deprecated Parameters

- **`CONSECUTIVE_NON_1TO1_PENALTY_MAX`**: Dynamic penalty mechanism removed from search stage. Exceptions now detected via final structural validation, not dynamic penalty sorting.

- **`MIN_QUALITY_THRESHOLD`**: Legacy parameter. Current single-stage DP maximizes total similarity (sum of edge weights); average similarity is computed post-hoc for reporting.

- **`non_one_to_one_raw_threshold`**: Edge-level filtering removed, replaced with node-level soft constraints.

### Parameter Tuning Suggestions

#### For Different Language Pairs

1. **High-similarity language pairs** (e.g., English-French):
   - Increase `node_relative_threshold` (e.g., 0.85-0.95) to reduce low-quality nodes
   - Result: Stricter search space, potentially better 1:1 alignments

2. **Low-similarity language pairs** (e.g., English-Chinese):
   - Decrease `node_relative_threshold` (e.g., 0.5-0.65) to allow more exploration
   - Result: More permissive search space, handling larger structural differences

3. **Pairs with large structural differences**:
   - Increase `consecutive_non_1to1_lookahead` (e.g., 7-10)
   - Result: Stricter detection of pathological alignment patterns

#### Performance vs. Quality Trade-offs

- **Highest quality**: Use default parameters, validated across many language pairs and text types
- **Fastest speed**: Increase `node_relative_threshold` or decrease `consecutive_non_1to1_lookahead`, reducing search space or exception detection
- **Special texts** (poetry, code, etc.): May need to adjust `node_relative_threshold` to fit special semantic structures

### Scientific Basis for Default Parameters

Current defaults are based on:

1. **$\theta = 0.8$**: Balanced point across different language pairs and text types, neither too strict nor too lenient
2. **$L = 5$**: Covers most actual consecutive non-1:1 exception patterns while avoiding over-reporting
3. **Soft constraints and node-level filtering**: More stable and easier to tune than edge-level thresholds

---

## Implementation Details

### Data Structures

#### Alignment Steps

Each alignment operation is represented as a step, containing:
- **Operation type**: `NO_SHIFT` (1:1), `SOURCE_AHEAD` (2:1), `TARGET_SPLIT` (1:2)
- **Source line range**: `[src_start, src_end]` (inclusive)
- **Target line range**: `[tgt_start, tgt_end]` (inclusive)
- **Similarity score**: Average similarity of this operation

#### DP State

- **DP table**: `dp[(i,j)]` stores the maximum cumulative score reaching node $(i,j)$
- **Parent pointer table**: `parent[(i,j)]` stores the predecessor node reaching this node
- **Edge weights**: Pre-computed similarity weights for all stable edges

### Key Algorithm Steps

#### Stage 0: Hard Constraint Computation
Enumerate all $(i,j)$ pairs, check if they satisfy:
- Forward constraint: $\lceil i/2 \rceil \le j \le 2i$
- Backward constraint: $\lceil (n-i)/2 \rceil \le m-j \le 2(n-i)$

#### Stage 0: Soft Constraint Application
Apply node-level filtering to hard-constraint node set. First compute similarity distributions for all 1:1 operations, then evaluate whether each node's best outgoing operation exceeds the relative threshold.

#### Stage 0: Stable Reachable Set Computation
Repeatedly apply forward and backward reachability filtering until the node set no longer changes (usually converges in 2-3 iterations).

#### Stage 1: Single-Stage DP
Process nodes in order of $\text{level} = i+j$. For each node, attempt transfers from all possible parent nodes, retaining the maximum cumulative score.

#### Stage 1: Path Reconstruction and Operation Extraction
Backtrack parent pointers from $(n,m)$ to $(0,0)$, obtaining the node sequence. Differences $(\Delta i, \Delta j)$ between adjacent nodes determine operation type.

#### Stage 2: Structural Validation
Scan the final operation sequence for consecutive different non-1:1 operations. Report as exceptions if distance does not exceed lookahead window.

### Key Performance Optimizations

1. **Similarity caching**: Avoid recomputing similarities for identical line pairs
2. **Constraint pruning**: Hard and soft constraints significantly reduce nodes to consider
3. **Stable reachable set**: Forward/backward filtering further ensures only meaningful nodes are processed
4. **Single-path search**: DP directly finds the optimal path without enumerating multiple candidates

---

## Innovative Aspects

### Single-Stage DP Design
Compared to traditional two-stage or multi-stage methods, single-stage DP achieves:
- **Search simplification**: Directly maximize the objective function (total similarity — sum of edge weights) without intermediate conversions
- **Operation fairness**: All operation types (1:1, 2:1, 1:2) use uniform similarity computation and weighting schemes
- **Exception separation**: Structural exception detection separated from search stage to final validation, ensuring search quality

### Node-Level Soft Constraints
Compared to traditional edge-level or single-exception rules:
- **Flexibility**: Adaptively filter weak nodes based on context
- **Interpretability**: Node filtering decisions based on actual quality (node_score) vs. reference score (element_best) comparisons
- **Stability**: Relative thresholds (not absolute) make parameters more robust across different language pairs and text types

### Post-Processing Repair Strategy
- **Descending application**: Avoid index drift, ensuring repair correctness
- **Multi-candidate generation**: Provide hard split, soft split, and insert options for 2:1 operations to increase success rate
- **Placeholder mechanism**: Synthetic lines do not participate in global calculations, preventing semantic space pollution
